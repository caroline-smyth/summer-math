\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{todonotes}
\usepackage{bbm}


\title{Thoughts}
\author{Caroline Smyth}
\date{June 2025}

\begin{document}

\maketitle

\section{Introduction} Can we introduce branching Brownian motion to rank-based models and does this result in a tractable mean field PDE?
For the rank-based particle systems we'd be able to simulate the dynamics in python, and the mean-field PDE may admit explicit steady state solutions. 
\section{Background}
We start from the SDE 
\begin{equation}
    dX_t = b(t, X_t)dt + \sigma(t, X_t)dW_t
\end{equation}
where $W_t$ is a Brownian motion. We can write $b(t, X_t) = \mu X_t$, $\mu \in \mathbb{R}$ and $\sigma(t, X_t) = \sigma X_t$, $\sigma \geq 0$.
Let $\sigma > 0$ and $\mu \in \mathbb{R}$. The geometric Brownian motion with parameters $\sigma$ and $\mu$ is the solution of the stochastic differential equation 
\begin{equation}
    dX_t = \mu X_t d_t +  \sigma X_t dW_t
\end{equation}
We solve this equation by applying It\^{o}'s formula to $log X_t$ and find that
\begin{equation}
    X_t = X_0 \exp (\sigma W_t + (\mu - \frac{\sigma^2}{2})t)
\end{equation}
Then write this as $X_t = X_0 g(t, W_t)$ 

?? $x = W_t$?\\
Then, using It\^{o}'s Lemma, we can rewrite the solution using the partials of g:
\\ $\partial_t g= (\mu - \frac{1}{2}\sigma^2)g$\\
$\partial_x g = \sigma g$\\
$\partial_{xx}g = \sigma^2 g$\\
\begin{align*}{}
    g(t, W_t) &= g(0, W_0)+ \int_0^t \partial_s g(s, W_s)ds + \int_0^t \partial _x g(s, W_s)d Ws + \frac{1}{2} \int_0^t \partial_{xx}^2 (s,W_s) (dWs)^2\\
    &= g(0, W_0) + \int_0^t((\mu - \frac{1}{2}\sigma^2 g(s, W_s)) + \frac{1}{2}g(s, W_s)ds))ds + \int_0^t\sigma g(s, W_s)dWs\\
     &= g(0, W_0) + \int_0^t\mu g(s, W_s) ds + \int_0^t \sigma g(s, W_s) dW_s 
\end{align*}

Since $X_t = X_0 g(t, W_t)$, $X_0 = X_0 g(0, W_0$ and $X_s = X_0 g(s, W_s)$, 
\begin{align*}
    X_t &= X_0 g(0, W_0) + X_0\int_0^t \mu g(s, W_s)ds + X_0\int_0^t\sigma g(s, W_s)dW_s\\
    &= X_0 + \int_0^t\mu X_0 g(s, W_s) ds + \int_0^t \sigma X_0 g(s, W_s) dW_s\\
    &= X_0 + \int_0^t \mu X_s ds + \int_0^t \sigma X_s dW_s\\
    dX_t &= \mu X_t dt + \sigma X_t dW_t
\end{align*}
So we've come full circle - the solution to this SDE is geometric Brownian motion.\\
Now we look at
\begin{align*}
    dX_t &= b(t, X_t)dt + \sigma(t, X_t)dW_t\\
    d\langle X \rangle_t &= (b(t, X_t)dt)^2 + b(t, X_t)\sigma(t, X_t)dtdWt + \sigma^2(t, X_t)d \langle W \rangle _ t \\
    &= 0 + 0 + \sigma^2(t, X_t)d \langle W \rangle _ t\\
    &= \sigma^2(t, X_t)dt
\end{align*}
Using It\^{o}'s formula, we want to show what $df(X_t)$ goes to if f $C^2$ decays fast enough.
\begin{align*}
    df(X_t) &= f'(X_t)dX_t + \frac{1}{2}f''(X_t)d\langle X \rangle_t\\
    &=f'(X_t)(b(t, X_t)dt + \sigma(t, X_t)dW_t) + \frac{1}{2}f''(X_t)\sigma^2(t, X_t)dt\\
    &=(f'(X_t)b(t, X_t) + \frac{1}{2}f''(X_t)\sigma^2(t, X_t))dt + f'(X_t)\sigma(t, X_t)dW_t
\end{align*}
Take It\^{o}'s integral from $[0, t]$ to find 
\begin{equation*}
    f(X_t) = f(X_0) + \int_0^t(b(s, X_s)f'(X_s) + \frac{1}{2}\sigma^2(s, X_s)f''(X_s))ds + \int_0^tf'(X_s)\sigma(s, X_s)dWs
\end{equation*}
We want to show that the stochastic integral  $\int_0^tf'(X_t)\sigma(t, X_t)$ is a martingale w.r.t. the filtration $\mathcal{F}_s$ generated by the Brownian motion $W_s$, which will allow us to write $f'(X_t)\sigma(t, X_t)dW_t$ as $dM_t$, which will conveniently disappear during the application of It\^{o}'s integral to $f(X_t)$. A process $(X_t)_{t\geq 0}$ is a martingale if, in a fixed probability space $(\Omega, \mathcal{F}, (\mathcal{F}_t), P)$,
\begin{enumerate}[label=(\roman*)]
    \item $X_t$ is adapted w.r.t the filtration $\mathcal{F}_s$ generated by $W_s$
    \item $X_t \in L^2$ for every $t \geq 0$, i.e. $\mathbb{E}[|X_t|^2] < \infty$
    \item For every $0 \leq s < t$, $\mathbb{E}[X_t | \mathcal{F}_s] = X_s$
\end{enumerate}
So in the case of $\int_0^tf'(X_s)\sigma(s, X_s)dW_s$, we first check for adaptedness. The $X_s$, is adapted to $\mathcal{F}_s$, the filtration generated by Brownian motion $W_s$. Functions of adapted processes are also adapted provided they are nice, i.e. continuous and bounded, which we assume $f'$ and $\sigma$ to be. So $f'(X_s)$ and $\sigma(s, X_s)$ are adapted. \\

We now want to check if the integrand satisfies square integrability, i.e. that
\begin{equation*}
    \mathbb{E}[\int_0^t(f'(X_s)\sigma(s, X_s))^2ds] < \infty
\end{equation*}
Assuming linear growth conditions, this is finite, so holds. (SHOW COMPLETELY?) \\
Because the integral meets these conditions, check if it meets the fundamental martingale property that $\mathbb{E}[M_t | \mathcal{F}_s] = M_s$. 
\begin{align*}
    \mathbb{E}[M_t | \mathcal{F}_s] &= \mathbb{E}[\int_0^tf'(X_t)\sigma(t, X_t)dW_t |\mathcal{F}_s]\\
    &=\mathbb{E}[\int_0^sf'(X_t)\sigma(t, X_t)dW_t + \int_s^tf'(X_t)\sigma(t, X_t)dW_t | \mathcal{F}s]\\
    &= \mathbb{E}[\int_0^sf'(X_t)\sigma(t, X_t)dW_t | \mathcal{F}_s] + \mathbb{E}[\int_s^tf'(X_t)\sigma(t, X_t)dW_t | \mathcal{F}s] \text{EXPLAIN THIS??}\\
    &=\mathbb{E}[M_s | \mathcal{F}_s]+ \mathbb{E}[\int_s^t f'(X_t)\sigma(t, X_t) |\mathcal{F}_s]\\
    &=M_s + 0
\end{align*}
So then that's a martingale, back to It\^{o}'s integral. 
Using It\^{o}'s integral (CHECK THIS AGAIN), we find:
\begin{align*}
    f(X_t) &= f(X_0) + \int_0^t(b(s, X_s)f'(X_s) + \frac{1}{2}\sigma^2(s, X_s)f''(X_s))ds + Mt\\
    \mathbb{E}[f(X_t)] &= \mathbb{E}[f(X_0) \int_0^t(b(s, X_s)f'(X_s) + \frac{1}{2}\sigma^2(s, X_s)f''(X_s))ds + M_t] \text{INCLUDE FILTRATION HERE?}\\
    &= \mathbb{E}[f(X_0) +\int_0^t(b(s, X_s)f'(X_s) + \frac{1}{2}\sigma^2(s, X_s)f''(X_s))ds + M_0] \\
    &= \mathbb{E}[f(X_0)+ \int_0^t(b(s, X_s)f'(X_s) + \frac{1}{2}\sigma^2(s, X_s)f''(X_s))ds]
\end{align*}
Using the conditional expectation allows us to say
\begin{align*}
    \mathbb{E}[f(X_t)] = \int_\mathbb{R} f(y)p(t, y)dy
\end{align*}
Then using It\^{o}'s again, represent the RHS as
\begin{align*}
    \int_\mathbb{R} f(y)p(t, y)dy &= \int_\mathbb{R} f(y)p(0, y)dy + \int_0^t\int_\mathbb{R} (b(s,y =)f'(y) + \frac{1}{2}\sigma^2(s, y)f''(y))p(s, y)dy ds\\
    \int_\mathbb{R} f(y)p(t, y)dy - \int_\mathbb{R} f(y)p(0, y)dy &= \int_0^t\int_\mathbb{R} (b(s,y =)f'(y) + \frac{1}{2}\sigma^2(s, y)f''(y))p(s, y)dy ds\\
\end{align*}
Do some integration by parts (FIGURE THIS OUT). Let $p(0, x) = \delta_{x_0}$ Hence $p$ satisfies the Kolmogorov forward equation (Fokker-Planck)(EXPLAIN THE DIFFERENCE BETWEEN THEM):
\begin{equation*}
    \partial_tp = -\partial_x(b(t, x)p(t, x)) + \partial_{xx}^2(\frac{1}{2}\sigma^2(t, x)p(t, x))
\end{equation*}
$L$ is the generator of SOMETHING. Then the partial of the probability density function w.r.t $t$ can be written as
\begin{equation*}
    \partial_tp = \mathcal{L}_t^*p
\end{equation*}
$\mathcal{L}_t^*$ is the adjoint of the generator of $X$. In general, a generator of a function $f(x)$ is defined by
\begin{align*}
    \mathcal{L}_tf(x) &= \lim_{h \downarrow 0}\frac{1}{h}(\mathbb{E}[f(X_{t+h})|X_t = x] - f(x))\\
    &= \lim_{h \downarrow 0}(\mathbb{E}_xf(X_{t+h} - f(x))
\end{align*}
Again, using It\^{o}'s lemma, we have
\begin{align*}
    f(X_{t+h}) &= f(X_t) + \int_t^{t+h}f'(X_s)dXs + \frac{1}{2}\int_t^{t_h}f''(X_s)x\langle X \rangle s \\
    &=f(X_t) + \int_t^{t+h}f'(X_s)b(s, X_s) ds + \sigma(s, W_s)dWs) + \frac{1}{2}\int_t^{t+h}f''(X_s)\sigma^2(s, X_s)ds\\
    &=f(X_t) + \int_t^{t+h}f'(X_s)b(s, X_s) + \frac{1}{2}\int_t^{t+h}f''(X_s)\sigma^2(s, X_s)ds + \int_t^{t+h}f'(X_s)\sigma(s, X_s)dWs
\end{align*}
The last component, $\int_t^{t+h}f'(X_s)\sigma(s, X_s)dWs$ is, by the same process as earlier (DO AGAIN?), a martingale - a measurable, adapted process that meets the necessary conditional expectation requirement lala, which means that it goes to zero because it's taken from $t, t$. So then we resume with that term equal to 0. 
\begin{align*}
    f(X_{t+h}) &=f(X_t) + \int_t^{t+h}f'(X_s)b(s, X_s) + \frac{1}{2}f''(X_s)\sigma^2(s, X_s)ds\\
    \mathbb{E}[f(X_{t+h}) | X_t= x] &= \mathbb{E}[f(X_t) + \int_t^{t+h}f'(X_s)b(s, X_s) + \frac{1}{2}f''(X_s)\sigma^2(s, X_s)ds | X_t = x]\\
    &= \mathbb{E}[f(x) + \int_t^{t+h}f'(X_s)b(s, X_s) + \frac{1}{2}f''(X_s)\sigma^2(s, X_s)\\
    \mathbb{E}[f(X_{t+h}] - f(x) &= \mathbb{E}[f(x) + \int_t^{t+h}f'(X_s)b(s, X_s) + \frac{1}{2}f''(X_s)\sigma^2(s, X_s) - f(x)\\
    \frac{1}{h}\mathbb{E}[f(X_{t+h}] -f(x)&=\frac{1}{h} \mathbb{E}[\int_t^{t+h}f'(X_s)b(s, X_s) + \frac{1}{2}f''(X_s)\sigma^2(s, X_s)]\\
    &= \frac{1}{h} \mathbb{E}[f'(X_t)b(t_, X_t) + \frac{1}{2}f''(X_t)\sigma^2(t, X_t)]h\\
    &= \mathbb{E}[f'(X_t)b(t_, X_t) + \frac{1}{2}f''(X_t)\sigma^2(t, X_t) | X_t = x]\\
    \mathcal{L}_t^*f(x) &=f'(x)b(t_, x) + \frac{1}{2}f''(x)\sigma^2(t, x)
\end{align*}
\todo{Mean Value Theorem }

Can we represent $\partial_tp $ as
\begin{equation}
    \partial_tp = \mathcal{L}_t^*p + \alpha p + \beta p^2
\end{equation}
and use data to fit for the coefficients $\alpha$ and $\beta$, $\alpha$ being the killing term and $\beta$ as the branching term? This is sort of the big question

\subsection{Adding branching to rank based models}
We need to define branching in this situation -- we can take it to mean that when a firm at rank $k$ dies, a new one is born at rank $n$, the smallest rank in the reverse order statistics construction, so that at any given time, the values of the rank processes represent the values of the original processes arranged in descending order. 
\begin{equation*}
    \max_{1 \leq i \leq n}X_i(t) =: X_{(1)}(t) \geq X_{(2)}(t) \geq \ldots \geq X_{(n-1)}(t) \geq X_{(n)}(t) := \min_{1 \leq i \leq n}X_i(t)
\end{equation*}
Banner section 2 - SDE for log-sizes
Consider real constants $\gamma, \; \sigma_1 > 0, \ldots, \sigma_n > 0$ and $g_1, \ldots, g_n$ satisfying the conditions 
\begin{equation*}
    g_1 <0, \; g_1 + g_2, \ldots, g_1 + \cdots + g_{n-1} <0 , \; g_1 + \cdots + g_n = 0
\end{equation*}
We look at the system of stochastic differential equations 
\begin{equation*}
    dY_i(t) = (\sum_{k=1}^n g_k \mathbbm{1}_{\mathcal{Q}_k^{(i)}}(Y(t))+ \gamma)dt \; + \sum_{k=1}^n\sigma_k \mathbbm{1}_{\mathcal{Q}_k^{(i)}}(Y(t)) \cdot dW_i(t), \; Y_i(0) = \bar{y_i}
\end{equation*}
From the Guyon and Henry-Labordere 13.2: Branching introductions have been first introduced to give a probabilistic representation of the KPP PDE and more generally of semilinear PDEs of the type
\begin{align*}
    \partial_tu + \mathcal{L}u+ \beta (t) (\sum_{k=0}^\infty p_ku^k - u) &= 0 \; \; \text{in [0, T) } \times \mathbb{R}^d \\
    u(T, x) &= g(x)
\end{align*}
with $\beta \geq 0 $. The probabilistic interpretation of this equation: let a single particle start at the origin, follow an It\^{o} diffusion with generator $\mathcal{L}$, after a mean $\beta(\cdot)$ exponential time (independent of $X$) die and produce $k$ descendents with probability $p^k$. Then the descendents perform independent It\^{o} diffusions on $\mathbb{R}^d$ with the same generator from their birth locations, repeat until mean $\beta(\cdot)$ exponential times, etc. This process is called a $d$-dimensional branching diffusion with a branching rate $\beta(\cdot)$. $\beta$ can be 
Rank dynamics sec 3

Stationary law and Fokker-Planck sec 4

\section{More background to write up}
Significance of continuous semimartingales, which are stochastic processes that can be decomposed into a continuous local martingale and continuous finite variation process. Emerge in ranking processes (Banner and Ghomrasni) 

Generator is effectively a derivative of a stochastic process - how much it moves in an infinitesimal amount of time 

Empirical CDF which gives a continuum view of the firm positions rather than having to look at discrete measures. Also as $n \rightarrow \infty$, can converge to a deterministic PDE like the porous medium equation.

Indicator functions give the rank of each firm. 

Collisions occur when two firms meet at the same rank (size?). This is managed through reflection rather than arbitrary movement after the collision. When $X_k = X_{k+1}$

How do portfolio generating functions from B-G relate to this?

Where does the $b(1 - \frac{i-1}{n}dt)$ param come from? 


\end{document}
